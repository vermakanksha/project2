{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten number classification using MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset containing 70,000 images of handwritten number from 0 to 9.\n",
    "It has been pre-processed and is considered as the first step to have hands on experience working with tensorflow for deep learning, often termed as the Hello World! for deep learning and artificial neural network.\n",
    "The dataset has been already split into training, validation and test subset.\n",
    "\n",
    "A training subset is used to create a basic model where it learns from the available records about how to deal with similar kind of data.\n",
    "A validation subset is used to make prediction on the basis of the model that we have created using training subset. These predictions are compared with the original targets to know how our model is performing. Then we keep on making changes in the parameters used in our model to get higher accuracy by comparing different results till the time we are satisfied with the accuracy level of our model.\n",
    "A test subset is used to make final predictions based on the model we settled for after validation. It is believed that after testing the model, there should be no changes made in the parameters in order to avoid overfitting.\n",
    "\n",
    "Overfitting: it occurs when a model is made so perfect on the basis of the records which are available with us, that it gives vague results when new records are introduced. This happens because the model covers each and every data point of the given dataset, which is of no use when it has to predict records from new dataset. Because those new data points would not fit the model.\n",
    "Underfitting: it occurs when a model covers very few data points while creating the model and therefore the accuracy level is very low. This can be understood by imagining a linear model created for a data which requires a non-linear model, it is bound to underfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset has been flattened into a vector of length 784 (28x28). The vector contains float values between 0 and 1, where a value close to 0 is almost white and the value close to 1 is almost black on the grayscale.\n",
    "The following code shows the vector of length 784 for the first image of the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a classification problem where the target variable has 10 categories [0,1,2,3,4,5,6,7,8,9], one hot encoding is used for labels. This means that if the first image is of 5, then the first list of labels will be [0,0,0,0,0,1,0,0,0,0]. Similarly, if the third image is of 9, then the third list of labels will be [0,0,0,0,0,0,0,0,0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the number of images in each subset\n",
    "Train: 55,000\n",
    "Test: 10,000\n",
    "Validation: 5,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[9456].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x130e95ac8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCOr/AVIrk/r72sCvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12e8ce550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[9457].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1310f6048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAg/ENR/AHZK1ho9EFADAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131005668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[9457].reshape(28,28),cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.262. Validation loss: 0.113. Validation accuracy: 96.70%\n",
      "Epoch 2. Mean loss: 0.091. Validation loss: 0.084. Validation accuracy: 97.70%\n",
      "Epoch 3. Mean loss: 0.054. Validation loss: 0.083. Validation accuracy: 97.52%\n",
      "Epoch 4. Mean loss: 0.034. Validation loss: 0.074. Validation accuracy: 97.78%\n",
      "Epoch 5. Mean loss: 0.023. Validation loss: 0.068. Validation accuracy: 98.06%\n",
      "Epoch 6. Mean loss: 0.017. Validation loss: 0.079. Validation accuracy: 98.06%\n",
      "End of training.\n",
      "Training time: 200.915274143219 seconds\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Using same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 1000\n",
    "\n",
    "# Resetting any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Declaring placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and \n",
    "# the first hidden layer.\n",
    "# Using get_variable in order to make use of the default TensorFlow \n",
    "# initializer which is Xavier.\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "# Choosing ReLu as the activation function.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first and second hidden layers.\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the first and the second hidden layers. Again, we use ReLu.\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "# Weights and biases for the third linear combination.\n",
    "# This is between the second and third hidden layers.\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the second and the third hidden layers. Again, we use ReLu.\n",
    "outputs_3 = tf.nn.relu(tf.matmul(outputs_2, weights_3) + biases_3)\n",
    "\n",
    "# Weights and biases for the final linear combination.\n",
    "# That's between the third hidden layer and the output layer.\n",
    "weights_4 = tf.get_variable(\"weights_4\", [hidden_layer_size, output_size])\n",
    "biases_4 = tf.get_variable(\"biases_4\", [output_size])\n",
    "\n",
    "# Operation between the third hidden layer and the final output.\n",
    "# We are not using an activation function because we'll use the trick to include \n",
    "# it directly in the loss function. \n",
    "# This works for softmax and sigmoid with cross entropy.\n",
    "outputs = tf.matmul(outputs_3, weights_4) + biases_4\n",
    "\n",
    "# Calculating the loss function for every output/target pair.\n",
    "# The function used, combines softmax and cross entropy in a clever way, \n",
    "# which makes it both faster and more numerically stable (when dealing with \n",
    "# very small numbers). Naturally, the labels are the targets.\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "# Getting the average loss\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Defining the optimization step. \n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(mean_loss)\n",
    "\n",
    "# Getting a 0 or 1 for every input in the batch, indicating whether it outputs\n",
    "# the correct answer out of the 10.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "# Getting the average accuracy of the outputs.\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "# Declaring the session variable.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initializing the variables. Default initializer is Xavier.\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# Batching\n",
    "batch_size = 100\n",
    "\n",
    "# Calculating the number of batches per epoch for the training set.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# Basic early stopping. Setting a maximum number of epochs.\n",
    "max_epochs = 15\n",
    "\n",
    "# Keeping track of the validation loss of the previous epoch.\n",
    "# If the validation loss becomes increasing, we want to trigger early stopping.\n",
    "# We initially set it at some arbitrarily high number to make sure we don't \n",
    "# trigger it at the first epoch\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Creating a loop for the epochs. Epoch_counter is a variable which \n",
    "# automatically starts from 0.\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    # Keeping track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterating over the batches in this epoch.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train \n",
    "        # dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Running the optimization step and getting the mean loss for this batch.\n",
    "        # Feeding it with the inputs and the targets we just got from the \n",
    "        # train dataset\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Incrementing the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # So far curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "    # We want to find the average batch losses over the whole epoch\n",
    "    # The average batch loss is a good proxy for the current epoch loss\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # At the end of each epoch, getting the validation loss and accuracy\n",
    "    # Getting the input batch and the target batch from the validation dataset\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    # Running without the optimization step (simply forward propagate)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    # Printing statistics for the current epoch\n",
    "    # Epoch counter + 1, because epoch_counter automatically starts from 0, \n",
    "    # instead of 1\n",
    "    # Formatting the losses with 3 digits after the dot\n",
    "    # Formatting the accuracy in percentages for easier interpretation\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Triggering early stopping if validation loss begins increasing.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # Storing this epoch's validation loss to be used as previous validation \n",
    "    # loss in the next iteration.\n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "# Not essential, but it is nice to know when the algorithm stopped working \n",
    "# in the output section, rather than checking the kernel\n",
    "print('End of training.')\n",
    "\n",
    "#Adding the time it took the algorithm to train\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.95%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "# Test accuracy is a list with 1 value, so we want to extract the value \n",
    "# from it, using x[0]\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# Printing the test accuracy formatted in percentages\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the model we have created on any new images of handwritten number and can classify those images with almost 98 percent accuracy, which is appreciable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
