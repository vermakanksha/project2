{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1.\tIntroduction to Neural Networks\n",
    "2.\tComponents of Training an Algorithm\n",
    "3.\tTypes of Machine Learning\n",
    "4.\tSupervised Machine Learning sub-types\n",
    "5.\tThe Linear Model\n",
    "6.\tThe Objective Function\n",
    "7.\tTypes of Loss Function\n",
    "8.\tGradient Descent\n",
    "9.\tTensorflow\n",
    "10.\t MNIST \n",
    "11.\t MNIST Solution Outline\n",
    "12.\t Action Plan for Deep Neural Network \n",
    "13.\t Prediction Accuracy\n",
    "14.\t Code  \n",
    "15.\t Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks\n",
    "\n",
    "The model is often termed as a Black Box, where input is fed to generate output. For example, if we have to predict the weather for tomorrow, we will have to feed some input variables like humidity, temperature and precipitation for past few days and the output generated will be the forecast for tomorrow.\n",
    "To be confident about the output from our model, one essential step is training the model through which it understands how to make sense of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Algorithm has four components:\n",
    "\n",
    "1)\tData – refers to the historical data available related to the problem that is to be solved.\n",
    "\n",
    "2)\tModel – the most basic is the linear model, but as we go deeper into machine learning, it has more complicated models that fit the data more accurately.\n",
    "\n",
    "3)\tObjective function – refers to a function that determines how closely, the outputs generated from the model and the targets available, are related to each other. In other words, the objective is to minimize the error or to maximize the reward.\n",
    "\n",
    "4)\tOptimization Algorithm – our model assigns certain weights to every input variable in order to generate outputs; these weights have to be optimized by trying different values. The optimized weights help in generating outputs with least error.\n",
    "\n",
    "In order to get the best model, we feed data into the model and compare Objective function. Then we vary the parameters and repeat all the steps till the time we reach a point where the model’s accuracy stops differing.\n",
    "\n",
    "\n",
    "Machine Learning is better than applying a set of instructions for the machine to follow because machine learning works on goals. For example, through machine learning, a coffee machine will be given a goal to make coffee without giving any set of instructions. It will learn through trial and error, therefore it will try thousands of recipes and it could become capable of making the best coffee ever made, even better than humans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Machine Learning:\n",
    "\n",
    "•\tSupervised – in supervised machine learning, we provide the algorithm with inputs and its corresponding desired outputs. It learns to produce as close outputs to the ones that we desire.\n",
    "\n",
    "•\tUnsupervised – in unsupervised machine learning, we don’t state the goal for the algorithm, we just feed inputs. Here we want the algorithm to find some underlying logic to separate the data into clusters.\n",
    "\n",
    "•\tReinforcement – in reinforcement machine learning, the algorithm is trained by rewarding it for doing better. For example, high scores are the rewards for doing better in a game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning can be divided into two sub-types:\n",
    "\n",
    "1)\tClassification – where the output that we want to generate is in categories. For example – happy customers, sad customers.\n",
    "\n",
    "2)\tRegression – where the output that we want to generate is numeric. For example – sales of a company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear model\n",
    "\n",
    "The linear model is represented as f(x) = xw + b\n",
    "\n",
    "Where, f(x) is the function of x, which is also equal to output y.\n",
    "\n",
    "x is the input variable\n",
    "\n",
    "w is the weight\n",
    "\n",
    "b is the bias\n",
    "\n",
    "In the above equation, we have considered only one input variable, but in real life problems there are multiple inputs that determine multiple outputs.\n",
    "\n",
    "One important thing to note is that once we have the number of inputs and number of outputs fixed, we have the weights and biases matrix created for it, then it doesn’t matter how many rows of data we feed into our model as the model will work the same for as much data as we want to work with.\n",
    "\n",
    "The linear equation is extremely important in machine learning and serves as the basis for building neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective function\n",
    "\n",
    "There are two types of objective functions:\n",
    "\n",
    "1)\tLoss function – refers to dealing with problems where we need to minimize the errors.\n",
    "\n",
    "2)\tReward function – these are the opposite of loss functions and deal with problems like reinforcement learning where the goal is to maximize the objective function value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on supervised machine learning problem, therefore we will discuss loss functions in detail.\n",
    "\n",
    "## There are two types of commonly used loss functions:\n",
    "\n",
    "L2-norm loss – used for regression problems in supervised learning.\n",
    "\n",
    "L2-norm = sum (y-t)^2\n",
    "\n",
    "where, y is the output generated through model\n",
    "\n",
    "and t is the target, which is the desired output\n",
    "\n",
    "The lower the error, the lower is the loss.\n",
    "\n",
    "\n",
    "\n",
    "Cross-entropy loss – used for classification problems in supervised learning.\n",
    "\n",
    "We are taking an example of dealing with three categories, that is, cats, dogs and horse, denoting the categories in numeric terms.\n",
    "\n",
    "y = [0.4,0.4,0.2]\n",
    "\n",
    "t = [0,1,0]\n",
    "\n",
    "The loss function here will be\n",
    "\n",
    "L(y,t) = -0 x In0.4 - 1 x In0.4 - 0 x In0.2\n",
    "\n",
    "= - 1 x In0.4\n",
    "\n",
    "= 0.92\n",
    "\n",
    "“In” is used to denote natural log.\n",
    "\n",
    "Similarly when,\n",
    "\n",
    "y = [0.1,0.2,0.7]\n",
    "\n",
    "t = [0,0,1]\n",
    "\n",
    "L(y,t) = -0 x In0.1 - 0 x In0.2 - 1 x In0.7\n",
    "\n",
    "= - 1 x In0.7\n",
    "\n",
    "= 0.36\n",
    "\n",
    "\n",
    "\n",
    "The lower the loss, the more accurate our model is. As we can see from our example also, that the second model with lower loss is better than the first model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "It is the simplest and most fundamental optimization algorithm.\n",
    "\n",
    "Update rule: x(i+1) = xi + eta x f’(xi)\n",
    "\n",
    "where, eta is the learning rate.\n",
    "\n",
    "The rule to set the learning rate is:\n",
    "\n",
    "•\tIt should be high enough, so that it reaches closest minimum in rational amount of time, with no unnecessary computation.\n",
    "\n",
    "•\tIt should be low enough, so that it doesn’t oscillate without reaching minimum.\n",
    "\n",
    "Few takeaways of gradient descent:\n",
    "\n",
    "•\tThrough trial and error we can find the minimum.\n",
    "\n",
    "•\tEvery trial is better than previous trial.\n",
    "\n",
    "•\tLearning rate should be optimal.\n",
    "\n",
    "•\tWe should stop updating once we have converged, i.e., when X(i+1) - Xi = 0.001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "The Google Brain Team has developed Tensorflow for machine learning. They first developed it for internal use but by the end of 2015, they released it for the public.\n",
    "\n",
    "Tensorflow uses both CPU and GPU of the computer system, which helps in higher computational power. Google also introduced TPU, which is Tensorflow Processing Unit, to further improve computation.\n",
    "\n",
    "Tensorflow is mostly focused on neural networks, performing better than sklearn. Other than neural networks, sklearn is quiet powerful for other machine learning tasks.\n",
    "\n",
    "Tensorflow helps in creating fairly complicated models with a little amount of coding; this is the beauty of it.\n",
    "\n",
    "We use tf.placeholder to feed the data, which is inputs and targets. A placeholder is used because these values stay constant and will not change during the process. It will just be fed one by one during the iteration.\n",
    "\n",
    "Whereas, tf.Variable is used to feed the variables such as weights and biases. A variable is used because these values keep changing and updating for optimization of our model.\n",
    "\n",
    "The outputs are defined using the linear equation by performing a matrix multiplication of inputs and weights then adding biases to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "The MNIST problem is considered as the Hello World of machine learning. It is the first problem that beginners start working on. One advantage of working with MNIST dataset is that it has been pre-processed, which makes it easier to work on it. Seventy percent of solving a machine learning problem requires pre-processing of the data and rest thirty percent is creating a model, therefore while working with MNIST dataset, seventy percent of our work is already done.\n",
    "\n",
    "The MNIST dataset contains seventy thousand images of handwritten digits ranging from 0 to 1. Therefore it is a classification problem with ten classes. The best part of working on MNIST dataset is that it is a very visual problem, extremely common and the dataset is very big, as well as preprocessed. Yan LeCun, Corinna Cortes and Christopher Burges have developed the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Solution Outline\n",
    "\n",
    "Every image in the MNIST dataset is of 28 by 28 pixels.\n",
    "\n",
    "The approach in deep neural network:\n",
    "\n",
    "•\tEach photo consists of 784 pixels.\n",
    "\n",
    "•\tEach pixel is an input for our neural network.\n",
    "\n",
    "•\tEach pixel corresponds to the intensity of the color (0 is white and 1 is black).\n",
    "\n",
    "Our input layer will consist of 784 input units. We will work with three hidden layers, since a deep neural network consists of three or more hidden layers. We will have 1000 nodes in every hidden layer. The number of hidden layers and the number of nodes in every layer is a parameter chosen arbitrarily, since a very deep neural network requires high computational power, which is not possible because of limited resources available with me. The output layer will have 10 units, since we are working with 10 classes.\n",
    "\n",
    "We use one hot encoding for our targets. For example, 0 is written as [1,0,0,0,0,0,0,0,0,0] and the digit 5 is written as [0,0,0,0,0,1,0,0,0,0].\n",
    "\n",
    "Since, we want to get probabilities assigned to every class in our output, we will use softmax activation function between the last hidden layer and the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The action plan used for creating the deep neural network:\n",
    "\n",
    "•\tOutline the model and choose activation function.\n",
    "\n",
    "•\tDescribe placeholders, variables and related operations.\n",
    "\n",
    "•\tChoose appropriate advanced optimizers.\n",
    "\n",
    "•\tSplit the dataset into batches for faster learning.\n",
    "\n",
    "•\tInitialize all variables.\n",
    "\n",
    "•\tMake it learn.\n",
    "\n",
    "•\tTest the accuracy of the model.\n",
    "\n",
    "\n",
    "\n",
    "There are certain hyper-parameters, which we can keep changing to get higher accuracy, till the time we are satisfied.\n",
    "\n",
    "These are:\n",
    "\n",
    "1)\tWidth of the network\n",
    "\n",
    "2)\tDepth of the network\n",
    "\n",
    "3)\tActivation functions\n",
    "\n",
    "4)\tLearning rate\n",
    "\n",
    "5)\tBatch size\n",
    "\n",
    "\n",
    "\n",
    "The for loop that we use in our code inside the main for loop, follows a sequence of steps:\n",
    "\n",
    "1)\tLoads 100 inputs and 100 targets (batch_size = 100).\n",
    "\n",
    "2)\tOptimizes the algorithm and calculates the batch loss.\n",
    "\n",
    "3)\tRecords the loss for the iteration.\n",
    "\n",
    "4)\tStarts over with the next 100 batches.\n",
    "\n",
    "5)\tStops when the training set is exhausted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction accuracy \n",
    "– refers to the percentages of cases where the model assigned highest probability to the output that matched the target.\n",
    "\n",
    "The final accuracy of the model comes from forward propagating through the test dataset as otherwise we may over-fit.\n",
    "\n",
    "This is because we try different combinations of hyper-parameters to find the best combination, but these are the hyper-parameters that fit the validation set best.\n",
    "\n",
    "Fiddling with the hyper-parameters, we eventually start over-fitting the validation dataset.\n",
    "\n",
    "We don’t train on the validation dataset, but we adjust the model considering the results from validation.\n",
    "\n",
    "The test accuracy is the final accuracy of the model. Once we check the test accuracy, we are conceptually not allowed to change our model. If we do that, we start over-fitting the model to test dataset, which defeats our purpose of separating validation and test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwritten number classification using MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset containing 70,000 images of handwritten number from 0 to 9.\n",
    "It has been pre-processed and is considered as the first step to have hands on experience working with tensorflow for deep learning, often termed as the Hello World! for deep learning and artificial neural network.\n",
    "The dataset has been already split into training, validation and test subset.\n",
    "\n",
    "A training subset is used to create a basic model where it learns from the available records about how to deal with similar kind of data.\n",
    "A validation subset is used to make prediction on the basis of the model that we have created using training subset. These predictions are compared with the original targets to know how our model is performing. Then we keep on making changes in the parameters used in our model to get higher accuracy by comparing different results till the time we are satisfied with the accuracy level of our model.\n",
    "A test subset is used to make final predictions based on the model we settled for after validation. It is believed that after testing the model, there should be no changes made in the parameters in order to avoid overfitting.\n",
    "\n",
    "Overfitting: it occurs when a model is made so perfect on the basis of the records which are available with us, that it gives vague results when new records are introduced. This happens because the model covers each and every data point of the given dataset, which is of no use when it has to predict records from new dataset. Because those new data points would not fit the model.\n",
    "Underfitting: it occurs when a model covers very few data points while creating the model and therefore the accuracy level is very low. This can be understood by imagining a linear model created for a data which requires a non-linear model, it is bound to underfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset has been flattened into a vector of length 784 (28x28). The vector contains float values between 0 and 1, where a value close to 0 is almost white and the value close to 1 is almost black on the grayscale.\n",
    "The following code shows the vector of length 784 for the first image of the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a classification problem where the target variable has 10 categories [0,1,2,3,4,5,6,7,8,9], one hot encoding is used for labels. This means that if the first image is of 5, then the first list of labels will be [0,0,0,0,0,1,0,0,0,0]. Similarly, if the third image is of 9, then the third list of labels will be [0,0,0,0,0,0,0,0,0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the number of images in each subset\n",
    "Train: 55,000\n",
    "Test: 10,000\n",
    "Validation: 5,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[9456].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x131438860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCOr/AVIrk/r72sCvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ee6e400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[9457].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13168fda0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAg/ENR/AHZK1ho9EFADAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1314ee668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[9457].reshape(28,28),cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.262. Validation loss: 0.111. Validation accuracy: 96.78%\n",
      "Epoch 2. Mean loss: 0.092. Validation loss: 0.079. Validation accuracy: 97.56%\n",
      "Epoch 3. Mean loss: 0.055. Validation loss: 0.071. Validation accuracy: 97.92%\n",
      "Epoch 4. Mean loss: 0.035. Validation loss: 0.063. Validation accuracy: 98.12%\n",
      "Epoch 5. Mean loss: 0.024. Validation loss: 0.061. Validation accuracy: 98.36%\n",
      "Epoch 6. Mean loss: 0.017. Validation loss: 0.074. Validation accuracy: 98.02%\n",
      "End of training.\n",
      "Training time: 210.48023509979248 seconds\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Using same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 1000\n",
    "\n",
    "# Resetting any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Declaring placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and \n",
    "# the first hidden layer.\n",
    "# Using get_variable in order to make use of the default TensorFlow \n",
    "# initializer which is Xavier.\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "# Choosing ReLu as the activation function.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first and second hidden layers.\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the first and the second hidden layers. Again, we use ReLu.\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "# Weights and biases for the third linear combination.\n",
    "# This is between the second and third hidden layers.\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the second and the third hidden layers. Again, we use ReLu.\n",
    "outputs_3 = tf.nn.relu(tf.matmul(outputs_2, weights_3) + biases_3)\n",
    "\n",
    "# Weights and biases for the final linear combination.\n",
    "# That's between the third hidden layer and the output layer.\n",
    "weights_4 = tf.get_variable(\"weights_4\", [hidden_layer_size, output_size])\n",
    "biases_4 = tf.get_variable(\"biases_4\", [output_size])\n",
    "\n",
    "# Operation between the third hidden layer and the final output.\n",
    "# We are not using an activation function because we'll use the trick to include \n",
    "# it directly in the loss function. \n",
    "# This works for softmax and sigmoid with cross entropy.\n",
    "outputs = tf.matmul(outputs_3, weights_4) + biases_4\n",
    "\n",
    "# Calculating the loss function for every output/target pair.\n",
    "# The function used, combines softmax and cross entropy in a clever way, \n",
    "# which makes it both faster and more numerically stable (when dealing with \n",
    "# very small numbers). Naturally, the labels are the targets.\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "# Getting the average loss\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Defining the optimization step. \n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(mean_loss)\n",
    "\n",
    "# Getting a 0 or 1 for every input in the batch, indicating whether it outputs\n",
    "# the correct answer out of the 10.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "# Getting the average accuracy of the outputs.\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "# Declaring the session variable.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initializing the variables. Default initializer is Xavier.\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# Batching\n",
    "batch_size = 100\n",
    "\n",
    "# Calculating the number of batches per epoch for the training set.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# Basic early stopping. Setting a maximum number of epochs.\n",
    "max_epochs = 15\n",
    "\n",
    "# Keeping track of the validation loss of the previous epoch.\n",
    "# If the validation loss becomes increasing, we want to trigger early stopping.\n",
    "# We initially set it at some arbitrarily high number to make sure we don't \n",
    "# trigger it at the first epoch\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Creating a loop for the epochs. Epoch_counter is a variable which \n",
    "# automatically starts from 0.\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    # Keeping track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterating over the batches in this epoch.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train \n",
    "        # dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Running the optimization step and getting the mean loss for this batch.\n",
    "        # Feeding it with the inputs and the targets we just got from the \n",
    "        # train dataset\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Incrementing the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # So far curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "    # We want to find the average batch losses over the whole epoch\n",
    "    # The average batch loss is a good proxy for the current epoch loss\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # At the end of each epoch, getting the validation loss and accuracy\n",
    "    # Getting the input batch and the target batch from the validation dataset\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    # Running without the optimization step (simply forward propagate)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    # Printing statistics for the current epoch\n",
    "    # Epoch counter + 1, because epoch_counter automatically starts from 0, \n",
    "    # instead of 1\n",
    "    # Formatting the losses with 3 digits after the dot\n",
    "    # Formatting the accuracy in percentages for easier interpretation\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Triggering early stopping if validation loss begins increasing.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # Storing this epoch's validation loss to be used as previous validation \n",
    "    # loss in the next iteration.\n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "# Not essential, but it is nice to know when the algorithm stopped working \n",
    "# in the output section, rather than checking the kernel\n",
    "print('End of training.')\n",
    "\n",
    "#Adding the time it took the algorithm to train\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.87%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "# Test accuracy is a list with 1 value, so we want to extract the value \n",
    "# from it, using x[0]\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# Printing the test accuracy formatted in percentages\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the model we have created on any new images of handwritten number and can classify those images with almost 98 percent accuracy, which is appreciable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The deep neural network created in this project is capable of producing nearly 98% accuracy within 3.5 minutes, which is an appreciable model. Although the model used is fairly complicated, but there are other models built using Convolutional Neural Networks (CNNs) with an accuracy of more than 99.5% because CNNs work best with image data.\n",
    "\n",
    "But it could not be used for this this project, since it requires a system with very high computation power, which is presently not available with me.\n",
    "\n",
    "Tensorflow can be used to solve any data science problem be it regression, classification, time-series, clustering, etc. It is new and very popular in the field of data science at present, used by big companies like Facebook, AirBnb, Snapchat, DeepMind, Uber, IBM, Intel, Ebay, Twitter and obviously Google. This list of companies can go on and on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](first.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](second.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](third.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fourth.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fifth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sixth.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](seventh.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
